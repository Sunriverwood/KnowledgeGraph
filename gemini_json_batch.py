import os
import json
import time
from datetime import datetime, timedelta

from google import genai
from google.genai import types
from google.api_core import exceptions

# ================================
# é…ç½®åŒº
# ================================
# æ¯ä¸ªå°æ‰¹æ¬¡åŒ…å«çš„PDFæ–‡ä»¶æ•°é‡
BATCH_SIZE = 20
# å•ä¸ªæ‰¹æ¬¡çš„æœ€é•¿è½®è¯¢æ—¶é—´
BATCH_POLLING_TIMEOUT_SECONDS = 8 * 60 * 60
# çŠ¶æ€æŒä¹…åŒ–æ–‡ä»¶
STATE_FILE = "processing_state.json"


# é…ç½®ä»£ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
os.environ["http_proxy"] = "http://127.0.0.1:7890"
os.environ["https_proxy"] = "http://127.0.0.1:7890"


# ================================
# çŠ¶æ€ç®¡ç†å‡½æ•°
# ================================
def load_state():
    """åŠ è½½å¤„ç†çŠ¶æ€ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™åˆå§‹åŒ–ã€‚"""
    if os.path.exists(STATE_FILE):
        with open(STATE_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}


def save_state(state):
    """å°†å½“å‰å¤„ç†çŠ¶æ€ä¿å­˜åˆ°æ–‡ä»¶ã€‚"""
    with open(STATE_FILE, 'w', encoding='utf-8') as f:
        json.dump(state, f, indent=2, ensure_ascii=False)


# ================================
# æŒ‡ä»¤åŠ è½½å‡½æ•°
# ================================
def load_graph_instructions(filepath="ä»»åŠ¡ï¼šæ ¹æ®æ··åˆSchemaä»PDFæ„å»ºå¯ç›´æ¥å¯¼å…¥çš„çŸ¥è¯†å›¾è°±.md"):
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            return f.read()
    except FileNotFoundError:
        return None


# ================================
# æ ¸å¿ƒåŠŸèƒ½å‡½æ•° (é‡æ„å’Œå®ç°)
# ================================
def process_job_results(client, batch_job, state, output_folder):
    """
    ã€å·²å®ç°ã€‘å¤„ç†æˆåŠŸä½œä¸šçš„ç»“æœï¼Œå¹¶æ›´æ–°çŠ¶æ€æ–‡ä»¶ã€‚
    """
    print(f"  -> æ­£åœ¨å¤„ç†ä½œä¸š '{batch_job.name}' çš„ç»“æœ...")
    if not (batch_job.dest and batch_job.dest.file_name):
        print(f"  - âŒ é”™è¯¯ï¼šä½œä¸š '{batch_job.name}' æˆåŠŸï¼Œä½†æœªæ‰¾åˆ°è¾“å‡ºæ–‡ä»¶ã€‚")
        # å°†æ‰€æœ‰ä¸æ­¤ä½œä¸šç›¸å…³çš„ä»»åŠ¡æ ‡è®°ä¸ºå¤±è´¥
        for pdf_file, data in state.items():
            if data.get('batch_job_name') == batch_job.name:
                data.update({'status': 'failed_job_no_output', 'error': 'ä½œä¸šæˆåŠŸä½†æ— è¾“å‡ºæ–‡ä»¶'})
        return

    result_file_name = batch_job.dest.file_name
    try:
        print(f"  - ğŸ“¥ æ­£åœ¨ä¸‹è½½ç»“æœæ–‡ä»¶: {result_file_name}")
        file_content = client.files.download(file=result_file_name).decode('utf-8')

        # é€è¡Œè§£æ JSONL ç»“æœæ–‡ä»¶
        for line in file_content.strip().split('\n'):
            result = json.loads(line)
            original_pdf_key = result.get("key")

            # å¦‚æœæ‰¾ä¸åˆ°keyï¼Œåˆ™æ— æ³•å…³è”ï¼Œè·³è¿‡æ­¤è¡Œ
            if not original_pdf_key:
                print(f"  - âš ï¸ è­¦å‘Šï¼šåœ¨ç»“æœæ–‡ä»¶ä¸­å‘ç°ä¸€ä¸ªæ²¡æœ‰ 'key' çš„æ¡ç›®ã€‚")
                continue

            # æ£€æŸ¥åŸå§‹PDFæ˜¯å¦å­˜åœ¨äºçŠ¶æ€ä¸­
            if original_pdf_key not in state:
                print(f"  - âš ï¸ è­¦å‘Šï¼šç»“æœæ–‡ä»¶ä¸­çš„ key '{original_pdf_key}' ä¸åœ¨å½“å‰çŠ¶æ€è·Ÿè¸ªä¸­ã€‚")
                continue

            # å¤„ç†å•ä¸ªè¯·æ±‚çš„æˆåŠŸæƒ…å†µ
            if result.get("response"):
                output_filename = os.path.splitext(original_pdf_key)[0] + ".json"
                output_path = os.path.join(output_folder, output_filename)
                try:
                    # æå– JSON å†…å®¹
                    json_text = result["response"]["candidates"][0]["content"]["parts"][0]["text"]
                    # æ¸…ç†å¹¶éªŒè¯
                    cleaned_json = json_text.strip().replace("```json", "").replace("```", "").strip()
                    json_data = json.loads(cleaned_json)
                    with open(output_path, "w", encoding="utf-8") as f:
                        json.dump(json_data, f, ensure_ascii=False, indent=2)

                    state[original_pdf_key].update({'status': 'completed', 'output_path': output_path})
                    print(f"    - âœ… æˆåŠŸ: '{original_pdf_key}' çš„ç»“æœå·²ä¿å­˜åˆ° {output_path}")

                except (KeyError, IndexError, json.JSONDecodeError) as e:
                    state[original_pdf_key].update({'status': 'failed_parsing', 'error': f"è§£æç»“æœå¤±è´¥: {e}"})
                    print(f"    - âŒ å¤±è´¥: è§£æ '{original_pdf_key}' çš„ç»“æœæ—¶å‡ºé”™: {e}")

            # å¤„ç†å•ä¸ªè¯·æ±‚çš„å¤±è´¥æƒ…å†µ
            elif result.get("error"):
                error_message = result['error'].get('message', 'æœªçŸ¥é”™è¯¯')
                state[original_pdf_key].update({'status': 'failed_in_job', 'error': error_message})
                print(f"    - âŒ å¤±è´¥: å¤„ç† '{original_pdf_key}' æ—¶APIè¿”å›é”™è¯¯: {error_message}")

    except Exception as e:
        print(f"  - âŒ ä¸¥é‡é”™è¯¯: å¤„ç†ç»“æœæ–‡ä»¶ '{result_file_name}' æ—¶å‘ç”Ÿæ„å¤–: {e}")
        # å°†æ‰€æœ‰ä¸æ­¤ä½œä¸šç›¸å…³çš„ä»»åŠ¡æ ‡è®°ä¸ºå¤±è´¥
        for pdf_file, data in state.items():
            if data.get('batch_job_name') == batch_job.name:
                data.update({'status': 'failed_processing_results', 'error': str(e)})


def generate_final_report(state):
    """æ ¹æ®æœ€ç»ˆçŠ¶æ€ç”Ÿæˆæ¸…æ™°çš„å¤„ç†æŠ¥å‘Šã€‚"""
    successful_files = [f for f, data in state.items() if data.get('status') == 'completed']
    failed_files = [f for f, data in state.items() if 'failed' in data.get('status', '')]
    pending_files = [f for f, data in state.items() if
                     data.get('status') not in ['completed'] and 'failed' not in data.get('status', '')]

    print("\n" + "=" * 50)
    print("ğŸ“‹ æœ€ç»ˆå¤„ç†æŠ¥å‘Š")
    print("=" * 50)

    print(f"\nâœ… å¤„ç†æˆåŠŸ ({len(successful_files)} ä¸ªæ–‡ä»¶):")
    if successful_files:
        for f in successful_files: print(f"  - {f}")
    else:
        print("  - æ— ")

    print(f"\nâŒ å¤„ç†å¤±è´¥ ({len(failed_files)} ä¸ªæ–‡ä»¶):")
    if failed_files:
        for f in failed_files:
            error = state[f].get('error', 'æœªçŸ¥é”™è¯¯')
            print(f"  - {f} (åŸå› : {error})")
    else:
        print("  - æ— ")

    if pending_files:
        print(f"\nâ³ å¾…å¤„ç†/å¤„ç†ä¸­ ({len(pending_files)} ä¸ªæ–‡ä»¶):")
        for f in pending_files: print(f"  - {f}")

    print("\n" + "=" * 50)


# ================================
# ä¸»ç¨‹åº (å…¨æ–°å·¥ä½œæµ)
# ================================
def main():
    # 1. åˆå§‹åŒ–
    pdf_folder = "data"
    output_folder = "json"
    model_name = "models/gemini-2.5-pro"

    if not os.path.exists(pdf_folder): os.makedirs(pdf_folder)
    os.makedirs(output_folder, exist_ok=True)

    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key: raise ValueError("âŒ é”™è¯¯ï¼šè¯·è®¾ç½® GEMINI_API_KEY ç¯å¢ƒå˜é‡")

    client = genai.Client(api_key=api_key)
    instructions = load_graph_instructions()
    if not instructions:
        print("âŒ é”™è¯¯ï¼šæœªèƒ½åŠ è½½æŒ‡ä»¤æ–‡ä»¶ã€‚")
        return

    state = load_state()

    # 2. æ–‡ä»¶å‘ç°ä¸çŠ¶æ€åŒæ­¥
    print(" Fase 1: æ–‡ä»¶å‘ç°ä¸çŠ¶æ€åŒæ­¥...")
    current_pdfs = {f for f in os.listdir(pdf_folder) if f.lower().endswith(".pdf")}
    for pdf_file in current_pdfs:
        if pdf_file not in state:
            state[pdf_file] = {'status': 'pending_upload'}
    save_state(state)
    print("  - çŠ¶æ€åŒæ­¥å®Œæˆã€‚")

    # 3. ä¸Šä¼ å¾…ä¸Šä¼ çš„æ–‡ä»¶
    print("\n Fase 2: ä¸Šä¼ æ–°æ–‡ä»¶...")
    files_to_upload = [f for f, data in state.items() if data['status'] == 'pending_upload']
    if files_to_upload:
        for pdf_file in files_to_upload:
            pdf_path = os.path.join(pdf_folder, pdf_file)
            try:
                print(f"  - æ­£åœ¨ä¸Šä¼ : {pdf_file}")
                response = client.files.upload(file=pdf_path)
                state[pdf_file].update({
                    'status': 'uploaded',
                    'uploaded_file_uri': response.uri,
                    'uploaded_file_name': response.name
                })
            except Exception as e:
                state[pdf_file].update({'status': 'failed_upload', 'error': str(e)})
            finally:
                save_state(state)
    else:
        print("  - æ— æ–°æ–‡ä»¶éœ€è¦ä¸Šä¼ ã€‚")

    # 4. åˆ›å»ºæ‰¹å¤„ç†ä½œä¸š
    print("\n Fase 3: ä¸ºå¾…å¤„ç†æ–‡ä»¶åˆ›å»ºæ‰¹å¤„ç†ä½œä¸š...")
    requests_to_process = []
    files_for_this_batch = []  # è®°å½•å“ªäº›æ–‡ä»¶è¢«åŒ…å«åœ¨å°†è¦åˆ›å»ºçš„æ‰¹å¤„ç†ä¸­
    for pdf_file, data in state.items():
        if data['status'] == 'uploaded':
            requests_to_process.append({
                "key": pdf_file,
                "request": {
                    "contents": [{"role": "user", "parts": [{"text": instructions}, {
                        "file_data": {"mime_type": "application/pdf", "file_uri": data['uploaded_file_uri']}}]}],
                    "generationConfig": {"response_mime_type": "application/json"}
                }
            })
            files_for_this_batch.append(pdf_file)

    if requests_to_process:
        request_chunks = [requests_to_process[i:i + BATCH_SIZE] for i in range(0, len(requests_to_process), BATCH_SIZE)]
        files_chunks = [files_for_this_batch[i:i + BATCH_SIZE] for i in range(0, len(files_for_this_batch), BATCH_SIZE)]

        for i, (chunk, files_in_chunk) in enumerate(zip(request_chunks, files_chunks)):
            batch_requests_file = f"temp_batch_requests_{i}.jsonl"
            job_display_name = f"KG-Batch-{datetime.now().strftime('%Y%m%d-%H%M%S')}-{i + 1}"

            try:
                # å†™å…¥ä¸´æ—¶çš„ JSONL æ–‡ä»¶
                with open(batch_requests_file, "w", encoding="utf-8") as f:
                    for req in chunk:
                        f.write(json.dumps(req) + "\n")

                # ä¸Šä¼  JSONL æ–‡ä»¶
                print(f"  - æ­£åœ¨ä¸Šä¼ è¯·æ±‚æ–‡ä»¶ '{batch_requests_file}'...")
                batch_input_file = client.files.upload(
                    file=batch_requests_file,
                    config=types.UploadFileConfig(display_name=job_display_name, mime_type='jsonl')
                )

                # åˆ›å»ºæ‰¹å¤„ç†ä½œä¸š
                print(f"  - æ­£åœ¨åˆ›å»ºæ‰¹å¤„ç†ä½œä¸š '{job_display_name}'...")
                batch_job = client.batches.create(
                    model=model_name,
                    src=batch_input_file.name,
                    config={'display_name': job_display_name}
                )
                print(f"  - âœ… ä½œä¸šåˆ›å»ºæˆåŠŸ: {batch_job.name}")

                # æ›´æ–°çŠ¶æ€
                for pdf in files_in_chunk:
                    state[pdf].update({'status': 'processing', 'batch_job_name': batch_job.name})
                save_state(state)

            except Exception as e:
                print(f"  - âŒ åˆ›å»ºæ‰¹å¤„ç†ä½œä¸šå— {i + 1} æ—¶å¤±è´¥: {e}")
                # å°†æ­¤å—ä¸­çš„æ–‡ä»¶æ ‡è®°ä¸ºå¤±è´¥
                for pdf in files_in_chunk:
                    state[pdf].update({'status': 'failed_job_creation', 'error': str(e)})
                save_state(state)
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(batch_requests_file):
                    os.remove(batch_requests_file)
    else:
        print("  - æ— å¾…å¤„ç†æ–‡ä»¶éœ€è¦åˆ›å»ºæ–°ä½œä¸šã€‚")

    # 5. ç›‘æ§æ‰€æœ‰â€œå¤„ç†ä¸­â€çš„ä½œä¸š
    print("\n Fase 4: ç›‘æ§æ‰€æœ‰å¤„ç†ä¸­çš„ä½œä¸š...")
    active_job_names = {data['batch_job_name'] for data in state.values() if data.get('status') == 'processing'}

    if not active_job_names:
        print("  - å½“å‰æ— æ´»åŠ¨ä½œä¸šéœ€è¦ç›‘æ§ã€‚")
    else:
        start_times = {name: datetime.now() for name in active_job_names}
        while active_job_names:
            print(f"  - æ­£åœ¨ç›‘æ§ {len(active_job_names)} ä¸ªæ´»åŠ¨ä½œä¸š...")
            for job_name in active_job_names:
                try:
                    job = client.batches.get(name=job_name)
                    print(f"  - ä½œä¸š '{job_name}' å½“å‰çŠ¶æ€: {job.state.name} ({time.strftime('%Y-%m-%d %H:%M:%S')})")
                except Exception as e:
                    print(f"  - è·å–ä½œä¸š '{job_name}' çŠ¶æ€æ—¶å‡ºé”™: {e}")
            sleep_interval = 600
            time.sleep(sleep_interval)
            finished_jobs = set()

            for job_name in list(active_job_names):
                # æ£€æŸ¥è¶…æ—¶
                elapsed = datetime.now() - start_times.get(job_name, datetime.now())
                if elapsed.total_seconds() > BATCH_POLLING_TIMEOUT_SECONDS:
                    print(f"â° ä½œä¸š '{job_name}' è¶…æ—¶ï¼Œæ­£åœ¨å°è¯•å–æ¶ˆ...")
                    try:
                        client.batches.cancel(name=job_name)
                    except exceptions.NotFound:
                        pass

                    for pdf, data in state.items():
                        if data.get('batch_job_name') == job_name:
                            data.update({'status': 'failed_timeout', 'error': 'æ‰¹å¤„ç†ä½œä¸šè¿è¡Œè¶…æ—¶'})
                    save_state(state)
                    finished_jobs.add(job_name)
                    continue

                # è·å–ä½œä¸šçŠ¶æ€
                try:
                    job = client.batches.get(name=job_name)
                    if job.state.name in ('JOB_STATE_SUCCEEDED', 'JOB_STATE_FAILED', 'JOB_STATE_EXPIRED',
                                          'JOB_STATE_CANCELLED'):
                        print(f"  -> ä½œä¸š '{job.name}' å·²å®Œæˆï¼ŒçŠ¶æ€: {job.state.name}")
                        if job.state.name == 'JOB_STATE_SUCCEEDED':
                            process_job_results(client, job, state, output_folder)
                        else:
                            error_detail = str(job.error) if job.error else f"ä½œä¸šä»¥çŠ¶æ€ {job.state.name} ç»“æŸ"
                            for pdf, data in state.items():
                                if data.get('batch_job_name') == job_name:
                                    data.update({'status': f'failed_{job.state.name.lower()}', 'error': error_detail})

                        save_state(state)
                        finished_jobs.add(job_name)
                except exceptions.NotFound:
                    print(f"  - âš ï¸ ä½œä¸š '{job_name}' åœ¨APIä¾§æœªæ‰¾åˆ°ï¼Œå¯èƒ½å·²è¢«åˆ é™¤ã€‚å°†å…¶æ ‡è®°ä¸ºå¤±è´¥ã€‚")
                    for pdf, data in state.items():
                        if data.get('batch_job_name') == job_name:
                            data.update({'status': 'failed_job_not_found', 'error': 'ä½œä¸šåœ¨APIä¾§ä¸¢å¤±'})
                    save_state(state)
                    finished_jobs.add(job_name)
                except Exception as e:
                    print(f"  - âŒ ç›‘æ§ä½œä¸š '{job_name}' æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    # é¿å…æ— é™å¾ªç¯ï¼Œæš‚æ—¶ä¸ä»æ­¤è½®ç›‘æ§ä¸­ç§»é™¤ï¼Œä¸‹æ¬¡å†è¯•

            active_job_names -= finished_jobs
            if active_job_names:
                print(f"  - ä»æœ‰ {len(active_job_names)} ä¸ªä½œä¸šåœ¨è¿è¡Œä¸­ï¼Œå°†åœ¨ {sleep_interval}såå†æ¬¡æ£€æŸ¥...")

    # 6. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    print("\n Fase 5: æ‰€æœ‰ä½œä¸šå¤„ç†å®Œæ¯•ï¼Œç”ŸæˆæŠ¥å‘Š...")
    generate_final_report(state)


if __name__ == "__main__":
    main()